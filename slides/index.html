<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>JuliaCon 2017 | Turing.jl</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Material design icons -->
		<link rel="stylesheet" href="kai/icons.css">

		<!-- Kai's customization -->
		<link rel="stylesheet" href="kai/my-style.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-markdown data-state="homepage">
					<textarea data-template>
						# Turing.jl

						*A fresh approach to probabilistic programming in Julia*

						<i class="material-icons">person</i> Kai Xu <i class="material-icons">schedule</i> 22 June 2017

						<p><i class="material-icons">group</i> Hong Ge &amp; Zoubin Ghahramani</p><!-- .element: class="fragment" -->

						<p><i class="material-icons">school</i> Machine Learning Group, University of Cambridge</p><!-- .element: class="fragment" -->
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						## Outline

						- What is probabilistic programming?
						- Why Julia?
						- Key features
						<!-- - Comparisons against other frameworks -->
						- How Turing.jl works?
						- Live demo
						- Q&A
					</textarea>
				</section>

				<section>
					<section data-markdown>
						<textarea data-template>
							### What is probabilistic programming?

							- Probabilistic programming is general-purpose programming with intrinsic support of non-deterministic statements.
							- Such programming languages are called probabilistic programming languages (PPLs).
							- One of the main applications of PPLs is probabilistic modelling, a popular field of machine learning.
						</textarea>
					</section>
					<section>
						<div class="grid">
							<div class="column">
								<ul>
								<li>Two types of PPLs</li>
									<ul>
										<li>Standalone:</li>
										<ul>
											<li>BUGS</li>
											<li>Stan</li>
										</ul>
										<li>Embedded-in:</li>
										<ul>
											<li>Probabilistic C in C</li>
											<li>Anglican in Clojure</li>
											<li>WebPPL in JS</li>
											<li>Turing.jl in <i class="material-icons">star</i><b>Julia</b><i class="material-icons">star</i></li>
										</ul>
									</ul>
								</ul>
							</div>
							<div class="column">
								<pre class="julia"><code data-trim data-noescape julia>
									using Turing

									@model gdemo(x) = begin
									  s ~ InverseGamma(2, 3)
									  m ~ Normal(0, sqrt(s))
									  x[1] ~ Normal(m, sqrt(s))
									  x[2] ~ Normal(m, sqrt(s))
									  s, m
									end

									modelf = gdemo([1.5, 2])
									alg1 = PG(50, 1000)
									chn1 = sample(modelf, alg)
									alg2 = HMC(1000, 0.2, 3)
									chn2 = sample(modelf, alg)
								</code></pre>
								<small>Code 1: Simple Turing.jl workflow</small>
							</div>
						</div>
					</section>
					<section>
						<div class="grid">
							<small>Looking closely, we can see the probablistic features we mentioned before in Turing.jl ...</small>
							<div class="column">
								<pre class="julia"><code data-trim data-noescape julia>
									using Turing

									@model gdemo(x) = begin
									  s ~ InverseGamma(2, 3)
									  m ~ Normal(0, sqrt(s))
									  x[1] ~ Normal(m, sqrt(s))
									  x[2] ~ Normal(m, sqrt(s))
									  s, m
									end

									modelf = gdemo([1.5, 2])
									alg1 = PG(50, 1000)
									chn1 = sample(modelf, alg)
									alg2 = HMC(1000, 0.2, 3)
									chn2 = sample(modelf, alg)
								</code></pre>
								<small>Code 1: Simple Turing.jl workflow</small>
							</div>
							<div class="column">
								<ul>
									<li>Non-deterministic?</li>
									<ul>
										<li>Distributions</li>
									</ul>
									<li>Language?</li>
									<ul>
										<li><code>@model</code> macro</li>
										<li><code>~</code> notaion</li>
									</ul>
									<li>Machine learning?</li>
									<ul>
										<li>Sampling methods</li>
										<ul>
											<li>SMC, PG, HMC, NUTS, Gibbs ...</li>
										</ul>
									</ul>
								</ul>
							</div>
						</div>
					</section>
				</section>

				<section data-markdown>
					<textarea data-template>
						### Why Julia?

						- Rich statistical libraries
							- Distributions.jl has a rich distributions
						- Meta-programming
							- Turing's compiler, i.e. `@model` and `~`
						- Coroutines
							- Particle Gibbs (PG) implementation
						- Automatic differentiation (AD)
							- Hamiltonian Monte Carlo (HMC) implementation
							- Generic typing help AD work with distributions
					</textarea>
				</section>

				<section>
					<section data-markdown>
						<textarea data-template>
							### Key features
							- Universal probabilistic programming with an ituitive modelling interface embedded in friendly Julia
							- PG sampler for distributions involving discrete variables and for stochastic control flows
							- HMC sampler for differentiable distributions
							- <i class="material-icons">new_releases</i> Compositional MCMC interface
							- <i class="material-icons">new_releases</i> Resumption of MCMC chains
							- <i class="material-icons">check_box_outline_blank</i> More novel samplers, other inference methods, sampling from user-defined models, ...

							<small><i class="material-icons">new_releases</i> = new releases, <i class="material-icons">check_box_outline_blank</i> = next steps</small>
						</textarea>
					</section>
					<section>
						<pre class="julia"><code data-trim data-noescape julia>
							using Turing

							@model gdemo(x, K) = begin
							    m = Vector{Real}(K);
							    for i = 1:K
							        m[i] ~ Normal(0, 25)
							    end
							    s ~ InverseGamma(2, 3)
							    N = length(x); z = tzeros(Int, N)
							    for i = 1:N
							        z[i] ~ Categorical(1/K*ones(K))
							        x[i] ~ Normal(m[z[i]], sqrt(s))
							    end
							    z, s, m
							end

							modelf = gdemo([1.1, 1.0, 0.9, 2.1, 2.2], 2)
							alg = <mark>Gibbs(1000, PG(50, 1, :z), HMC(1, 0.2, 3, :m, :s))</mark>
							chn = sample(modelf, alg)
						</code></pre>
						<small>Code 2: Compositional MCMC interface</small>
					</section>
					<section>
						<pre class="julia"><code data-trim data-noescape julia>
							using Turing

							@model gdemo(...) = begin
							  ...
							end

							modelf = gdemo(...)
							<mark>chn1 = sample(modelf, HMC(1000, 1, 0.2, 3; save_state=true))</mark>
							<mark>chn2 = sample(modelf, NUTS(1000, 0.65; resume_from=chn1))</mark>
						</code></pre>
						<small>Code 3: Resumption of MCMC chains</small>
					</section>
				</section>

				<section>
					<section>
						<h3>How Turing.jl works?</h3>
						<div class="grid">
							<div class="column resize-font-75">
								<b><u>Key ML techniques</u></b>
								<ul>
									<li>Bayesian inference</li>
									<ul>
										<li>General framework for probabilistic modelling</li>
									</ul>
									<li>Sampling</li>
									<ul>
										<li>Particle filtering</li>
										<li>Markov chain Monte Carlo</li>
									</ul>
								</ul>
								<br>
								<b><u>Key Julia techniques</u></b>
								<ul>
									<li>Coroutines</li>
									<ul>
										<li>Particle based samplers</li>
									</ul>
									<li>Automatic differentiation</li>
									<ul>
										<li>Hamiltonian based samplers</li>
									</ul>
								</ul>
							</div>
							<div class="column resize-font-75">
								<b><u>Key system components</u></b>
								<ul>
									<li>Model <i>defined by</i> users</li>
									<ul>
										<li>Normal Julia program with modelling operations</li>
									</ul>
									<li>Sampler <i>specified by</i> users</li>
									<ul>
										<li>Need to interact with model</li>
									</ul>
									<li><code>VarInfo</code></li>
									<ul>
										<li>Key data structure</li>
										<li>Enable interactions between models and samplers</li>
										<li>Users don't see it</li>
									</ul>
									<li>Samples <i>returned to</i> users</li>
									<ul>
										<li>Embedded in <code>Chain</code> type</li>
									</ul>
								</ul>
							</div>

						</div>
					</section>

					<section data-background-color="#34495e">
						<!--
						Title: Workflow of Turing.jl: generating one sample
						participant Model
						participant VarInfo
						participant Sampler(s)
						participant Sample

						Note over Sampler(s): Require evaluating model
						Sampler(s)->VarInfo: +Embed info
						VarInfo->Model: Apply & run
						Note over Model: Evaluating
						Model->VarInfo: Executed & return
						VarInfo->Sampler(s): -Fetch result
						Note over Sampler(s): Get an evaluation of model
						Sampler(s)->VarInfo: Require output
						VarInfo->Sample: All info converted
						-->
						<img src="kai/diagram.svg"></img>
						<small>Figure 1: Workflow of Turing.jl: generating one sample</small>
					</section>

					<section data-markdown>
						<textarea data-template>
							### How Turing.jl uses coroutines
							- IS, SMC and PG represent each particle running in parallel using a coroutine
							- Each particle is essentially a copy of the model, i.e. a block of normal Julia code
							- Duplicating or killing a particle is cheap
							- Continuation and pausing of coroutine is frequently used because SMC and PG do model evaluation sequentially for each observation
							- Lead to state-of-the-art SMC and PG performance
						</textarea>
					</section>

					<section data-markdown>
						<textarea data-template>
							### How AD works in Turing.jl
							1. Amend variables that need gradient information into `Dual` numbers
							2. Set dual parts to `1` for each dimension
							3. Update variables in `VarInfo`
							4. Do model evalutation
							5. Fetch gradient information from returns
							  - Returned log-likelihood is in `Dual` type with gradient as its dual parts
						</textarea>
					</section>
				</section>

				<section data-markdown>
					<textarea data-template>
						# Live demo
						Time to see a live Turing.jl program ...
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						# Q&A
						Any question?
					</textarea>
				</section>



				<!-- <section>
					<div class="grid">
						<div class="column">
							left
						</div>
						<div class="column">
							right
						</div>
					</div>
				</section> -->

				<!-- <section data-markdown>
					<textarea data-template>
					</textarea>
				</section> -->

				<section data-markdown>
					<textarea data-template>
						# Thank you for listening <i class="material-icons md-96">sentiment_very_satisfied</i>
					</textarea>
				</section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				math: { mathjax: 'kai/MathJax-2.7.1/MathJax.js', config: 'TeX-AMS_HTML-full' },
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/math/math.js', async: true }
				]
			});

			// Redirect of homepage
			Reveal.addEventListener( 'homepage', function() {
				// Reveal.slide( 4 );
			}, false );

			Reveal.configure({ slideNumber: 'c/t' });
		</script>
	</body>
</html>
