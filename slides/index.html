<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>JuliaCon 2017 | Turing.jl</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Material design icons -->
		<link rel="stylesheet" href="kai/icons.css">

		<!-- Kai's customization -->
		<link rel="stylesheet" href="kai/my-style.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-state="homepage" data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">>
					<textarea data-template>
						# Turing.jl

						*A fresh approach to probabilistic programming in Julia*

						<i class="material-icons">person</i> Kai Xu <i class="material-icons">event</i> JuliaCon, 22 June 2017

						<i class="material-icons">group</i> Hong Ge &amp; Zoubin Ghahramani

						<i class="material-icons">school</i> Machine Learning Group, University of Cambridge

						*<p class="resize-font-66 align-right">https://github.com/yebai/Turing.jl</p>*

						Note:
						Hello everyone my name is Kai Xu. I'm from the Machine Learning Group of University of Cambridge.
						Today I'm here to give a talk to introduce a project I'm currently working on, which is call Turing.jl.
						It's a probablistic programming framework purely written in Julia.
						We currently have two main developers for the project, Hong Ge and myself, and we are both supervisoed by Prof. Zoubin Gharamani.
						All right, let's start.
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						## Outline

						- What is probabilistic programming?
						- Why Julia?
						- Key features
						<!-- - Comparisons against other frameworks -->
						- How Turing.jl works?
						- Live demo
						- Q&A
					</textarea>
				</section>

				<section>
					<section data-markdown>
						<textarea data-template>
							### What is probabilistic programming?

							- Probabilistic programming is general-purpose programming with intrinsic support of non-deterministic statements.
							- Such programming languages are called probabilistic programming languages (PPLs).
							- One of the main applications of PPLs is probabilistic modelling, a popular field of machine learning.
						</textarea>
					</section>
					<section>
						<div class="grid">
							<div class="column">
								<ul>
								<li>Two types of PPLs</li>
									<ul>
										<li>Standalone:</li>
										<ul>
											<li>BUGS</li>
											<li>Stan</li>
										</ul>
										<li>Embedded-in:</li>
										<ul>
											<li>Probabilistic C in C</li>
											<li>Anglican in Clojure</li>
											<li>Edward in Python</li>
											<li>WebPPL in JS</li>
											<li>Turing.jl in <i class="material-icons">star</i><b>Julia</b><i class="material-icons">star</i></li>
										</ul>
									</ul>
								</ul>
							</div>
							<div class="column">
								<p>
									$$
										\sigma^2 \sim \textit{Inv-Gamma}(2, 3) \\
										\mu \sim \textit{Normal}(0, \sigma) \\
										x \sim \textit{Normal}(\mu, \sigma)
									$$
								</p>
								<pre class="julia"><code data-trim data-noescape julia>
									@model gdemo(x) = begin
									  σ² ~ InverseGamma(2, 3)
									  μ  ~ Normal(0, sqrt(σ²))
									  x[1] ~ Normal(μ, sqrt(σ²))
									  x[2] ~ Normal(μ, sqrt(σ²))
									  σ², μ
									end
								</code></pre>
								<small>Code 1: Gaussian model with conjugate priors</small>
							</div>
						</div>
						<aside class="notes">
							Oh hey, these are some notes. They'll be hidden in your presentation, but you can see them if you open the speaker notes window (hit 's' on your keyboard).
						</aside>
					</section>
					<section>
						<div class="grid">
							<small>Looking closely, we can see the probablistic features we mentioned before in Turing.jl ...</small>
							<div class="column">
								<pre class="julia"><code data-trim data-noescape julia>
									using Turing

									@model gdemo(x) = begin
									  σ² ~ InverseGamma(2, 3)
									  μ  ~ Normal(0, sqrt(σ²))
									  x[1] ~ Normal(μ, sqrt(σ²))
									  x[2] ~ Normal(μ, sqrt(σ²))
									  σ², μ
									end

									modelf = gdemo([1.5, 2])
									alg1 = PG(50, 1000)
									chn1 = sample(modelf, alg)
									alg2 = HMC(1000, 0.2, 3)
									chn2 = sample(modelf, alg)
								</code></pre>
								<small>Code 2: Gaussian model with conjugate priors (inference step breakdown)</small>
							</div>
							<div class="column">
								<ul>
									<li>Non-deterministic?</li>
									<ul>
										<li>Distributions</li>
									</ul>
									<li>Language?</li>
									<ul>
										<li><code>@model</code> macro</li>
										<li><code>~</code> notaion (macro)</li>
									</ul>
									<li>Machine learning?</li>
									<ul>
										<li>Bayesian inference by sampling: SMC, PG, HMC, NUTS, Gibbs ...</li>
									</ul>
								</ul>
							</div>
						</div>
					</section>
				</section>

				<section data-markdown>
					<textarea data-template>
						### Why Julia?

						- Rich collections of statistical libraries
							- Distributions.jl covers common distributions
							- Mamba.jl has good utility functions to compute statistics of MCMC chains
						- Meta-programming
							- Turing's compiler, i.e. `@model` and `~`
						- Coroutines
							- Particle Gibbs implementation
						- Automatic differentiation (AD)
							- ForwardDiff.jl provides an easy-to-use `Dual` type
							- Hamiltonian Monte Carlo implementation
							- Generic typing help AD work with distributions
					</textarea>
				</section>

				<section>
					<section data-markdown>
						<textarea data-template>
							### Key features
							- Universal probabilistic programming with an ituitive modelling interface embedded in Julia
							- Support of models with discrete variables and stochastic control flows
							- HMC sampler for differentiable distributions
							- <i class="material-icons">new_releases</i> Compositional MCMC interface
							- <i class="material-icons">new_releases</i> Resumption of MCMC chains
							- <i class="material-icons">check_box_outline_blank</i> More novel samplers, other inference methods, sampling from user-defined models, ...

							<small><i class="material-icons">new_releases</i> = new releases, <i class="material-icons">check_box_outline_blank</i> = next steps</small>
						</textarea>
					</section>
					<section>
						<small>
							$$ \mu_k \sim \textit{Normal(0, 25)}, \sigma_k^2 \sim \textit{Inv-Gamma}(2, 3) $$
							$$ z_i \sim \textit{Cat}(1/K), x_i \sim \textit{Normal}(\mu_{z_i}, \sigma_{z_i}) $$
						</small>
						<pre class="julia"><code data-trim data-noescape julia>
							using Turing

							@model MoG(x, K) = begin
							    μ = Vector{Real}(K)
							    σ² = Vector{Real}(K)
							    for i = 1:K
							        μ[i] ~ Normal(0, 25)
							        σ²[i] ~ InverseGamma(2, 3)
							    end
							    N = length(x); z = tzeros(Int, N)
							    for i = 1:N
							        z[i] ~ Categorical(1/K*ones(K))
							        x[i] ~ Normal(μ[z[i]], sqrt(σ²[z[i]]))
							    end
							    z, σ², μ
							end

							modelf = MoG([1.1, 1.0, 0.9, 2.1, 2.2], 2)
							alg = <mark>Gibbs(1000, PG(50, 1, :z), HMC(1, 0.2, 3, :μ, :σ²))</mark>
							chn = sample(modelf, alg)
						</code></pre>
						<small>Code 3: Compositional MCMC interface</small>
					</section>
					<section>
						<pre class="julia"><code data-trim data-noescape julia>
							using Turing

							@model somemodel(...) = begin
							  ...
							end

							modelf = somemodel(...)
							<mark>chn1 = sample(modelf, HMC(1000, 1, 0.2, 3; save_state=true))</mark>
							<mark>chn2 = sample(modelf, NUTS(1000, 0.65; resume_from=chn1))</mark>
						</code></pre>
						<small>Code 3: Resumption of MCMC chains</small>
					</section>
				</section>

				<section>
					<section>
						<h4>How Turing.jl works?</h4>
						<div class="grid">
							<div class="column resize-font-75">
								<b><u>Key ML techniques</u></b>
								<ul>
									<li>Bayesian inference</li>
									<ul>
										<li>General framework for probabilistic modelling</li>
									</ul>
									<li>Sampling</li>
									<ul>
										<li>Particle filtering</li>
										<li>Markov chain Monte Carlo</li>
									</ul>
								</ul>
								<br>
								<b><u>Key Julia techniques</u></b>
								<ul>
									<li>Coroutines</li>
									<ul>
										<li>Particle based samplers</li>
									</ul>
									<li>Automatic differentiation</li>
									<ul>
										<li>Gradient based samplers</li>
									</ul>
								</ul>
							</div>
							<div class="column resize-font-75">
								<b><u>Key system components</u></b>
								<ul>
									<li>Model <i>defined by</i> users</li>
									<ul>
										<li>Normal Julia program with modelling operations</li>
									</ul>
									<li>Sampler <i>specified by</i> users</li>
									<ul>
										<li>Need to interact with model</li>
									</ul>
									<li><code>VarInfo</code></li>
									<ul>
										<li>Key data structure</li>
										<li>Enable interactions between models and samplers</li>
										<li>Users don't see it</li>
									</ul>
									<li>Samples <i>returned to</i> users</li>
									<ul>
										<li>Embedded in <code>Chain</code> type</li>
									</ul>
								</ul>
							</div>

						</div>
					</section>

					<section data-background-color="#ecf0f1">
						<!--
						Title: Workflow of Turing.jl: generating one sample
						participant Model
						participant VarInfo
						participant Sampler(s)
						participant Sample

						Note over Sampler(s): Require evaluating model
						Sampler(s)->VarInfo: +Embed info
						VarInfo->Model: Apply & run
						Note over Model: Evaluating
						Model->VarInfo: Executed & return
						VarInfo->Sampler(s): -Fetch result
						Note over Sampler(s): Get an evaluation of model
						Sampler(s)->VarInfo: Require output
						VarInfo->Sample: All info converted
						-->
						<img src="kai/diagram.svg">
						<small>Figure 1: Workflow of Turing.jl: generating one sample</small>
					</section>

					<section data-markdown>
						<textarea data-template>
							#### How particle filtering works in Turing.jl
							- IS, SMC and PG represent each particle running in parallel using a coroutine
							- Each particle is a "live" copy of the model, i.e. an ongoing execution of the model
							- Duplicating or killing a particle is cheap
							- Continuation and pausing of coroutine is frequently used because SMC and PG do model evaluation sequentially for each observation
							- Lead to state-of-the-art SMC and PG performance
						</textarea>
					</section>

					<section data-markdown>
						<textarea data-template>
							#### How HMC works in Turing.jl
							1. HMC algorithm needs the unormalized posterior of the model and the gradient of variables
							2. Execution of the model program with variables set as `ForwardDiff.Dual` gives both
							  - Unormalized posterior = real part of log-joint
							  - Gradient of variables = dual parts of log-joint
							3. Produce a sample candidate by simulating Hamiltonian dynamics with the leapfrog algorithm
							4. Accept or reject the sample candidate
						</textarea>
					</section>
				</section>

				<section data-markdown>
					<textarea data-template>
						# Live demo
						Time to see a live Turing.jl program ...
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
						# Q&A
						Any question?
					</textarea>
				</section>



				<!-- <section>
					<div class="grid">
						<div class="column">
							left
						</div>
						<div class="column">
							right
						</div>
					</div>
				</section> -->

				<!-- <section data-markdown>
					<textarea data-template>
					</textarea>
				</section> -->

				<section data-markdown>
					<textarea data-template>
						# Thank you for listening <i class="material-icons md-96">sentiment_very_satisfied</i>
					</textarea>
				</section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				math: { mathjax: 'plugin/MathJax-2.7.1/MathJax.js', config: 'TeX-AMS_HTML-full' },
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/math/math.js', async: true }
				]
			});

			// Redirect of homepage
			Reveal.addEventListener( 'homepage', function() {
//				 Reveal.slide( 5 );
			}, false );

			Reveal.configure({ slideNumber: 'c/t' });
		</script>
	</body>
</html>
